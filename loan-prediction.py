# -*- coding: utf-8 -*-
"""lo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lc3vpIY27knf6eBD-UHQF5EN033taB4i
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

# converting csv file into pandas dataframe
df=pd.read_csv(r'loan train.csv')

df.head()

# dropping loan id as it not needed for prediction
df=df.drop(columns=['Loan_ID'])

df.isnull().head()

# total income of applicant i.e applicant+coapplicant incomes is added as it will be more relevant
df['total income']=df['ApplicantIncome']+df['CoapplicantIncome']
# Now we can drop coapplicant income column
df=df.drop(columns=['CoapplicantIncome'])

k=df.isnull()

# heatmap representing where the null values are present
sns.heatmap(k,yticklabels=False)

# counts of null values per each column
df.isnull().sum()

sns.countplot('Loan_Status',data=df)

print((df['Loan_Status']=='Y').sum())
print((df['Loan_Status']=='N').sum())

sns.pairplot(data=df,vars=['ApplicantIncome','LoanAmount','total income','Loan_Amount_Term'],hue='Loan_Status',markers='.')
# using this pairplot the conclusions that can be made are
#1. There are many rejected points with very low applicant income similary for total income also
#2. For higher loan amount term and low applicant income also there are many rejected points
#3. For higher loan amount term and low loan amount also there are many rejected points

df.corr()
# Appliant income and total income are highly correlated
# The correlation coefficient between total income and loan amount is 0.62 which indicates people with high total income are mostly asking for high loan amount

sns.distplot(df['ApplicantIncome'],bins=50)

sns.distplot(df['LoanAmount'],bins=50)

sns.distplot(df['Loan_Amount_Term'],bins=50)
# This disribution is concentrated on only some points. So we can make it as a categorical variable

sns.distplot(df['total income'],bins=50)

sns.boxplot(data=df,y='ApplicantIncome',x='Loan_Status')

sns.boxplot(data=df,y='total income',x='Loan_Status')

sns.boxplot(data=df,y='LoanAmount',x='Loan_Status')

df.dtypes

sns.countplot('Gender',data=df,hue='Loan_Status')

# reaplcing missing values of gender with mode 
df['Gender'].value_counts()

df['Gender'].fillna(df['Gender'].mode()[0],inplace=True)

df['Married'].value_counts()

# If married there is a high chance of having coapplicant income also 
for i in range(0,len(df)):
    if(df['Married'][i]!='Yes' and df['Married'][i]!='No'):
        if(df['total income'][i]>df['ApplicantIncome'][i]):
            df['Married'][i]='Yes'
        else:
            df['Married'][i]='No'

df['Married'].isnull().sum()

df['Dependents'].value_counts()

sns.countplot('Dependents',data=df,hue='Married')

# Take dependents as 1 if married else 0 (just based on probability)
for i in range(0,len(df)):
    if(df['Dependents'][i] not in ['0','1','2','3+']):
        if(df['Married'][i]=='Yes'):
            df['Dependents'][i]='1'
        else:
            
            df['Dependents'][i]='0'

df['Self_Employed'].value_counts()

sns.countplot('Education',data=df,hue='Self_Employed')

# Most of the educated are not self employed
for i in range(0,len(df)):
    if(df['Self_Employed'][i] not in ['Yes','No']):
        if(df['Education'][i]=='Graduate'):
            df['Self_Employed'][i]='Yes'
        else:
            df['Self_Employed'][i]='No'

m=df['LoanAmount'].median()

# Replacing missing values of loan amount with median(as mean can be affected by outliers)
df['LoanAmount'].fillna(m,inplace=True)

df['Loan_Amount_Term'].value_counts()

# Replacing missing values of Loan_Amount_Term with mode
df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mode()[0],inplace=True)

df['Credit_History'].value_counts()

sns.countplot('Credit_History',data=df,hue='Loan_Status')

# If loan is accepted then replace missing values of credit history with 1 else with 0 
for i in range(0,len(df)):
    if(df['Credit_History'][i]!=0 and df['Credit_History'][i]!=1):
        if(df['Loan_Status'][i]=='Y'):
            df['Credit_History'][i]=1.0
        elif(df['Loan_Status'][i]=='N'):
            df['Credit_History'][i]=0.0

df.isnull().sum()

# In all the areas loans are getting accepted but in case of semi urban the acceptance rate is high
sns.countplot('Property_Area',data=df,hue='Loan_Status')

# removing outliers by looking at loan amount
df= df[df['LoanAmount']<400]

len(df)

# removing outliers by looking at total income
df= df[df['total income']<24000]
len(df)

# encoding object type features
df_cat=pd.get_dummies(df)

df_cat.shape

df_cat=df_cat.drop(columns=['Loan_Status_N','Loan_Status_Y'])
df_cat['Loan_Status']=df['Loan_Status']

# converting loan amount term to categorical
df_cat['Loan_Amount_Term']=df_cat['Loan_Amount_Term'].astype(object)
df_cat['Loan_Amount_Term'].dtype

sns.countplot('Loan_Amount_Term',data=df_cat,hue='Loan_Status')

df_cat=pd.get_dummies(df_cat)
df_cat.shape

df_cat.dtypes

# scaling all the variables such that mean is 0 and std is 1
df_cat['LoanAmount']=(df_cat['LoanAmount']-df_cat['LoanAmount'].mean())/df_cat['LoanAmount'].std()
df_cat['total income']=(df_cat['total income']-df_cat['total income'].mean())/df_cat['total income'].std()
df_cat['ApplicantIncome']=(df_cat['ApplicantIncome']-df_cat['ApplicantIncome'].mean())/df_cat['ApplicantIncome'].std()

df_cat=df_cat.drop(columns=['Loan_Status_N','Loan_Status_Y'])
df_cat['Loan_Status']=df['Loan_Status']

X=df_cat.drop(columns=['Loan_Status'])
y=df_cat['Loan_Status']
y=y.map({'Y':1,'N':0})

print(y.head())

#Splitting the model into train and test sets
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42)
len(X_train)

X_train.columns

# Model based feature selection using random forest model
from sklearn.ensemble import  RandomForestClassifier 
r= RandomForestClassifier(max_depth=4, random_state=42,n_estimators=8,max_features=int(pow(len(X_train.columns),0.5)))

r.fit(X_train,y_train)

k=r.feature_importances_
k

plt.barh(X_train.columns,k)

# selcting top 12 features based on feature importances 
from sklearn.feature_selection import SelectFromModel
select=SelectFromModel(r,max_features=12,threshold=-np.inf,prefit=True)

feature_idx = select.get_support()

feature_idx

X

feature_name = X.columns[feature_idx]

feature_name

X=select.transform(X)

X.shape

# After feature selection dividing into train and test sets
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=1)
len(X_train)

# buiding a logistic regression model
from sklearn.linear_model import LogisticRegression
l=LogisticRegression()

l.fit(X_train,y_train)

y_pred=l.predict(X_test)

# measuring accuracy of test set
((y_pred==y_test).sum())/len(X_test)

# measuring accuracy of training set
(((l.predict(X_train))==y_train).sum())/len(X_train)

# using confusion matrix to classify the predictions into TP,TN,FP,FN
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test,y_pred)

# knowing the recall and precision using classification report
from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred,target_names=['N','Y']))

# ROC curve for logistic regression
from sklearn.metrics import roc_curve 
fpr1, tpr1, thresholds1 = roc_curve(y_test, l.decision_function(X_test))
plt.plot(fpr1, tpr1, label="ROC Curve") 
plt.xlabel("FPR") 
plt.ylabel("TPR (recall)") 
# find threshold closest to zero 
close_zero = np.argmin(np.abs(thresholds1)) 
plt.plot(fpr1[close_zero], tpr1[close_zero], 'o', markersize=10,label="threshold zero", fillstyle="none", c='k', mew=2) 
plt.legend(loc=4)

# Building a SVC model
from sklearn.svm import SVC
svc=SVC()

svc.fit(X_train,y_train)

y_pr=svc.predict(X_test)

((y_pr==y_test).sum())/len(X_test)

(((svc.predict(X_train))==y_train).sum())/len(X_train)

from sklearn.metrics import confusion_matrix
confusion_matrix(y_test,y_pr)

from sklearn.metrics import classification_report
print(classification_report(y_test,y_pr,target_names=['N','Y']))

from sklearn.ensemble import RandomForestClassifier
ran= RandomForestClassifier(n_estimators=90, random_state=2,max_depth=3,max_features= 3)

ran.fit(X_train,y_train)

k=ran.predict(X_test)

((k==y_test).sum())/len(X_test)

(((ran.predict(X_train))==y_train).sum())/len(X_train)

from sklearn.metrics import confusion_matrix
confusion_matrix(y_test,k)

from sklearn.metrics import classification_report
print(classification_report(y_test,y_pr,target_names=['N','Y']))

# Combining ROC curves of both SVC and Logisticregression
from sklearn.metrics import roc_curve 
fpr, tpr, thresholds = roc_curve(y_test, svc.decision_function(X_test))
fpr1, tpr1, thresholds1 = roc_curve(y_test, l.decision_function(X_test))
plt.plot(fpr, tpr, label="ROC Curve svc") 
plt.plot(fpr1, tpr1, label="ROC Curve lr",c='r') 
plt.xlabel("FPR") 
plt.ylabel("TPR (recall)") 
# find threshold closest to zero 
close_zero = np.argmin(np.abs(thresholds))
close_zero1 = np.argmin(np.abs(thresholds1))
plt.plot(fpr[close_zero], tpr[close_zero], '*', markersize=10,label="threshold zero svc", fillstyle="none", c='k', mew=2) 
plt.plot(fpr1[close_zero1], tpr1[close_zero1], 'o', markersize=10,label="threshold zero lr", fillstyle="none", c='k', mew=2) 
plt.legend(loc=4)

# Finding the area under the ROC curves of both the models
from sklearn.metrics import roc_auc_score 
lr_auc = roc_auc_score(y_test, l.predict_proba(X_test)[:, 1]) 
svc_auc = roc_auc_score(y_test, svc.decision_function(X_test)) 
print("AUC for Logistic regression: ",lr_auc) 
print("AUC for SVC: ",svc_auc)

